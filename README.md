# Learning-from-demonstrations

### Generate Expert Demonstration 
 '''py
 python experiment/data_generation/fetch_data_generation.py
 '''

## Hindsight Experience Replay 
HER is used to enhance sample efficiency in reinforcement learning. It trains agents more efficiently by introducing additional target samples in the replay buffer and reutilizing the failure experience to achieve the goal.

## Parameter Setting
1. DDPG\
    The number of layers in the critic/actor networks - 'layers': 3\
    The number of neurons in each hidden layers - 'hidden': 256\
    Learning rate of Critic - 'Q_lr': 0.001\
    Learning rate of Actor - 'pi_lr': 0.001\
    Polyak averaging coefficient - 'polyak': 0.95
   
2. HER\
    Replay mode - 'replay_strategy': 'future'\
    The number of additional goals used for replay - 'replay_k': 4
   
3. Training\
    Per epoch - 'n_cycles': 50\
    Training batches per cycle - 'n_batches': 40\
    'batch_size': 256\
    'n_test_rollouts': 10, number of test rollouts per epoch\
    'bc_loss': 1, use the behavior cloning loss as an auxilliary loss\
    'q_filter': 1, use Q value filter on the Actor outputs\
    'num_demo': 100, number of expert demomonstration episodes\
    'demo_batch_size': 128, number of samples to be used from the demonstrations buffer\
    'prm_loss_weight': 0.001, primary loss\
    'aux_loss_weight':  0.0078, auxilliary loss(cloning loss)

## Interpretation of code
### ddpg.py
   
### her.py
   Hindsight Experience Replay (HER) was used to improve the efficiency of the sample data.
   - def learn(*, network, env, total_timesteps, ...)
     It is the core part of the training, initiating the environment, the policy network, etc. and then calling the function train()to train.
   - def train(*, policy, rollout_worker, evaluator,..., demo_file, **kwargs)
     Train a policy network for reinforcement learning in a distributed environment. It generates trajectory data by interacting with the environment and then trains in batches with these data, while logging and saving the trained models.

### her_sample.py
   Generate experience replay data.
  
### actor_critic.py
   It implements an Actor-Critic network and training. The policy network (Actor) generates actions, and the Q-value network (Critic) evaluates the Q-value for a given combination of observation, goal and action. These networks were utilised to train a RL algorithm to maximise the cumulative rewards of the policy.

### normalizer.py
   Two classes, Normalizer and IdentityNormalizer, are implemented to perform ‘Normalisation’ on the input data. 
   For Normalizer, normalise the input data to an approximate standard normal distribution, while supporting data synchronisation in a multi-process environment. It improves the training efficiency and stability of deep learning models. 
   For IdentityNormalizer, mapping the input data to a constant is simple. It divides the input data by a predefined standard deviation or multiplying the standardised data by the standard deviation to achieve data reduction.

### Rollout.py
   Implementation of the RolloutWorker in her.py，it is the training data generated by interaction with the environment.






   
