# Learning-from-demonstrations

### Generate Expert Demonstration 
 ```
 python experiment/data_generation/fetch_data_generation.py
 ```
### Training with demonstrations and her
```
python -m baselines.run --alg=her --env=FetchPickAndPlace-v1 --num_timesteps=7.5e6 --demo_file=/Path/to/demo_file.npz
```
Training in different environments by changing the variable ’--env= ', such as 'FetchPush-v1', 'FetchSlide-v1' and 'FetchPickAndPlace'.

### Hindsight Experience Replay (HER) 
HER is used to enhance sample efficiency in reinforcement learning. It trains agents more efficiently by introducing additional target samples in the replay buffer and reutilizing the failure experience to achieve the goal.

It needs Python 3.7 or higher, and the TensorFlow should not exceed 2.x. The training result video can be accessed from the directory 'result_video/' and then downloaded by clicking 'view raw'.

## Parameter Setting
1. DDPG\
    The number of layers in the critic/actor networks - 'layers': 3\
    The number of neurons in each hidden layers - 'hidden': 256\
    Learning rate of Critic - 'Q_lr': 0.001\
    Learning rate of Actor - 'pi_lr': 0.001\
   
2. HER\
    Replay mode - 'replay_strategy': 'future'\
    The number of additional goals used for replay - 'replay_k': 4
   
3. Training\
    Per epoch - 'n_cycles': 50\
    'batch_size': 256\
    'bc_loss_function': 1, use the behavior cloning loss as an auxilliary loss\
    'filter': 1, use Q value filter on the Actor outputs\
    'num_demo': 100, number of expert demomonstration episodes\
    'demo_batch_size': 128, number of samples to be used from the demonstrations buffer\
    'prm_loss_weight': 0.001, primary loss\
    'aux_loss_weight':  0.0078, auxilliary loss(cloning loss)

## Interpretation of code

### ddpg.py
   It implements DDPG algorithm, including Actor-Critic networks, experience replay and demonstration buuffers, neuural network models and training procedures.
    
### her.py
   Hindsight Experience Replay (HER) was used to improve the efficiency of the sample data.
   - def learn(*, network, env, total_timesteps, ...)
     It is the core part of the training, initiating the environment, the policy network, etc. and then calling the function train()to train.
   - def train(*, policy, rollout_worker, evaluator,..., demo_file, **kwargs)
     Train a policy network for reinforcement learning in a distributed environment. It generates trajectory data by interacting with the environment and then trains in batches with these data, while logging and saving the trained models.

### her_sample.py
   Generate experience replay data.
  
### actor_critic.py
   It implements an Actor-Critic network for training. The policy network (Actor) generates actions, and the Q-value network (Critic) evaluates the Q-value for a given combination of observation, goal and action. These networks were utilised to train a RL algorithm to maximise the cumulative rewards of the policy.

### normalizer.py
   Two classes, Normalizer and IdentityNormalizer, are implemented to perform ‘Normalisation’ on the input data. 
   For Normalizer, normalise the input data to an approximate standard normal distribution, while supporting data synchronisation in a multi-process environment. It improves the training efficiency and stability of deep learning models. 
   For IdentityNormalizer, mapping the input data to a constant is simple. It divides the input data by a predefined standard deviation or multiplying the standardised data by the standard deviation to achieve data reduction.

### Rollout.py
   Implementation of the RolloutWorker in her.py，it is the training data generated by interaction with the environment.






   
